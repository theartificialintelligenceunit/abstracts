{"config":{"separator":"[\\s\\-_,:!=\\[\\]()\\\\\"`/]+|\\.(?!\\d)"},"items":[{"location":"","level":1,"title":"Projects, etc.","text":"<ul> <li> <p> Text/String Detection &amp; Classification: Dispatches</p> <p>Contextual text/string detection and classification.</p> <p>README</p> </li> <li> <p> River Level Intelligence</p> <p>Daily and on-demand river level intelligence vis-à-vis Scotland's rivers. </p> <p>README</p> </li> <li> <p> Text/String Detection &amp; Classification: Eclectic</p> <p>Contextual text/string detection and classification.</p> <p>README</p> </li> <li> <p> A&amp;E</p> <p>Forecasting accident and emergency attendance in Scotland.</p> <p>README</p> </li> <li> <p> Orchestration &amp; Automation</p> <p>Illustrating continuous development &amp; delivery via a simple, efficient, cost effective, and secure end-to-end solution.</p> <p>README</p> </li> </ul> <p> </p> <p>Notes aiding project delivery:</p> <ul> <li> <p> Code of Practice</p> <p>Notes vis-à-vis operating norms, development &amp; delivery, security.</p> <p>[HUB]</p> </li> <li> <p> Developing &amp; Deploying ML/AI Dependent Systems</p> <p>Notes vis-à-vis designing &amp; developing artificial intelligence, machine learning, dependent solutions; project lifecycle, viability, feasibility.</p> <p>[HUB] </p> </li> </ul> <p> </p> <p> </p> <p> </p> <p> </p> <p> </p> <p> </p> <p> </p> <p> </p>","path":["Projects, etc."],"tags":[]},{"location":"sections/ae/","level":1,"title":"Accident &amp; Emergency","text":"<p>The focus herein is accident &amp; emergency attendance forecasting.  The focus is on Scotland's accident &amp; emergency centres; specifically those that provide weekly counts, presently the most granular level available via Public Health Scotland.</p> <p></p>","path":["A&E","Accident &amp; Emergency"],"tags":[]},{"location":"sections/ae/#in-brief","level":2,"title":"In brief","text":"CommentExpires / Decommissioning Intelligence Hub Hosts automatically updated measures, metrics, forecasts, etc. End of November 2025 Repositories Hub Hosts the project's git repositories.","path":["A&E","Accident &amp; Emergency"],"tags":[]},{"location":"sections/ae/data/","level":1,"title":"Data","text":"","path":["A&E","Data"],"tags":[]},{"location":"sections/ae/data/#sources","level":2,"title":"Sources","text":"<p>The accident &amp; emergency data source, alongside descriptions and supplementary information.<sup>1</sup><sup>, </sup><sup>2</sup></p> <p></p> Notes name Weekly Accident &amp; Emergency (A&amp;E) Activity, and Waiting Times modality Time Series brief description Every week, on a Tuesday, Public Health Scotland releases an updated CSV file consisting of (a) counts of weekly A&amp;E attendance at approximately 30 hospitals, and (b) waiting time metrics.  The weekly counts span Monday → Sunday.        <ul> <li>Assessment of Accident and Emergency (A&amp;E) Activity Statistics in Scotland</li> <li>An overview</li> <li>Scottish Health and Social Care Open Data</li> </ul> raw data access Weekly Accident &amp; Emergency Activity, and Waiting Times <ul> <li>Detailed data description.</li> <li>Hospital Codes</li> <li>Population Estimates</li> </ul> <p> </p> <p> </p> <p> </p> <p> </p> <p> </p> <ol> <li> <p>Scottish Health and Social Care Open Data ↩</p> </li> <li> <p>Five big problems the NHS in Scotland needs to fix ↩</p> </li> </ol>","path":["A&E","Data"],"tags":[]},{"location":"sections/automation/","level":1,"title":"Orchestrate &amp; Automate","text":"<p>An illustration of continuous integration, deployment, and delivery via an end-to-end solution that publishes the latest daily quantiles distributions of nitrogen dioxide levels; automatically &amp; daily.  The orchestration &amp; automation design considers/addresses efficiency, cost effectiveness, and security.</p> <p></p>","path":["Orchestrate<br>& Automate","Orchestrate &amp; Automate"],"tags":[]},{"location":"sections/automation/#in-brief","level":2,"title":"In brief","text":"CommentExpires / Decommissioning Intelligence Hub Hosts automatically updated metrics. End of November 2025 Repositories Hub Hosts the project's git repositories. Objective To aid upcoming/future data platform team, etc., projects.","path":["Orchestrate<br>& Automate","Orchestrate &amp; Automate"],"tags":[]},{"location":"sections/automation/background/","level":1,"title":"Background","text":"","path":["Orchestrate<br>& Automate","Background"],"tags":[]},{"location":"sections/automation/background/#about","level":2,"title":"About","text":"<p>The objective of this exercise is to illustrate how Amazon Web Services (AWS) tools enable the orchestration &amp; automation of end-to-end solutions that eliminate or minimise the manual steps of - data dependent - operations, tasks.  For example</p> <p>A solution that can</p> <p>Automatically (a) read data from one or more sources around the world, (b) inspect the data, address anomalies, and structure the data, (c) deliver structured data to defined points, (d) run sets of algorithms, e.g., business intelligence algorithms, on schedule, and (e) deliver a run's results to defined points.</p> <p>and is</p> <p><ul> <li>Secure.</li> <li>Developed within secure continuous integration, delivery, and deployment settings; this includes version control.</li> <li>In compliance with reproducibility and auditability mandates.</li> <li>Platform agnostic; subject to platform (a) interaction, and (b) programming interface constraints.</li> </ul></p>","path":["Orchestrate<br>& Automate","Background"],"tags":[]},{"location":"sections/automation/background/#illustration","level":2,"title":"Illustration","text":"<p>To illustrate orchestration &amp; automation via AWS tools, the exercise focuses on providing daily quantile distributions of nitrogen dioxide levels within a city.  The city in focus is Edinburgh, which has a few telemetric measuring devices across the city.</p> <p>The solution has two parts</p> <ol> <li>A one-off solution that sets up a data depository within Amazon, and subsequently delivers historical Scottish Air Quality nitrogen dioxide data to the depository; after cleaning and structuring. </li> <li>A solution designed to operate daily.  Each day it automatically reads and structures the latest nitrogen readings, conducts quantiles and extrema calculations, and delivers the calculations to a point where a graph's data interface automatically reads it.</li> </ol> <p>Instead of a graph, it could be a mobile app, an alert system, etc.</p> <p> </p> <p> </p> <p> </p> <p> </p>","path":["Orchestrate<br>& Automate","Background"],"tags":[]},{"location":"sections/automation/notes/","level":1,"title":"Orchestration &amp; Automation","text":"<p>The orchestration and automation outline.</p>","path":["Orchestrate<br>& Automate","Orchestration &amp; Automation"],"tags":[]},{"location":"sections/automation/notes/#architecture","level":2,"title":"Architecture","text":"<p>This illustration outlines the orchestration &amp; automation architecture; enlarge the illustration via the fit or zoom buttons, at the bottom of the illustration.  For more details about an item/icon, hover over the item/icon; a few icons have links to more details.  Assets, e.g., container images, are delivered to Amazon Web Services (AWS) via GitHub.</p> <p>The GitHub Organization excomputing hosts all the repositories of this project.  In line with the agnostic solutions request, all computations are via containers.  Container image registration is automatic, it occurs during pushes to GitHub, i.e., it is part of this project's continuous integration, delivery, &amp; deployment set-up; the registries are</p> <ul> <li>Amazon Elastic Container Registry</li> <li>GitHub Container Registry.</li> </ul> <p>A GitHub Actions event (a) builds an image of a repository, and (b) delivers the image to either or both registries; these steps automatically occur during on-master-push to GitHub.</p> <p> </p> <p> </p> <p> </p> <p> </p>","path":["Orchestrate<br>& Automate","Orchestration &amp; Automation"],"tags":[]},{"location":"sections/dispatches/","level":1,"title":"Token Classification: Dispatches<sup>1</sup>","text":"<p>This project develops a text/string detection &amp; classification model by fine-tuning large language model architectures. The model detects and classifies text/strings vis-à-vis a set of classes.  The model depends on Reuters International News Bulletins of a specific time period, therefore the data's timeliness and context will impact its performance in various settings.</p> <p>The modelling package, and the supplementary packages, are adaptable components.</p> <p></p>","path":["Token Classification:<br>Dispatches","Token Classification: Dispatches"],"tags":[]},{"location":"sections/dispatches/#in-brief","level":2,"title":"In brief","text":"CommentExpires / Decommissioning Intelligence Hub Hosts model &amp; data details, and [automatically updated] dynamic model card components. End of November 2025 app A simple interface for interacting with the developed model; an endless number of interface designs are possible.  The classes in focus are  organisation, person, time, geographic entity, geo-political entity [excluding geographic items] Repositories Hub Hosts the project's git repositories. <ol> <li> <p>Herein, token classification is the detection and classification of text or strings via fine-tuned large language model architectures. Each model detects and classifies text or strings of vis-à-vis a set of classes. ↩</p> </li> </ol>","path":["Token Classification:<br>Dispatches","Token Classification: Dispatches"],"tags":[]},{"location":"sections/dispatches/background/","level":1,"title":"Background","text":"","path":["Token Classification:<br>Dispatches","Background"],"tags":[]},{"location":"sections/dispatches/background/#aim","level":2,"title":"Aim","text":"<p>In brief.  Organisations identify or detect words/strings of interest within documents or data blobs for a variety of purposes, e.g., knowledge graph development, text redaction, criminal investigations, etc.  Organisations may opt for manual word detection if off-the-shelf solutions cannot address their needs.  For example, most redaction software solutions focus on detecting and redacting words/strings of a narrow set of categories, and it is rarely possible to adapt the solution to the detection of words/strings of bespoke categories.</p> <p>A viable solution-strategy involves the creation of custom models via named entity recognition machine learning algorithms; also known as token classification algorithms. These algorithms can be trained by domain or problem, leading to compact, domain specific, word/string detection models.</p> <p>This project publishes an adaptable token classification modelling set-up, i.e., artificial/machine learning engineers can fork and adapt the repositories to their (a) token classification problem, (b) development environment, and (c) the range of language models that they would like to try.</p> <p> </p> <p> </p> <p> </p> <p> </p>","path":["Token Classification:<br>Dispatches","Background"],"tags":[]},{"location":"sections/dispatches/case/","level":1,"title":"Applications","text":"<p>This section outlines a sample \\((a)\\) problem statement, \\((b)\\) corresponding outcome expectations/underlying aims, and \\((c)\\) deployment goal vis-à-vis token classification model development.</p> <p></p>","path":["Token Classification:<br>Dispatches","Applications"],"tags":[]},{"location":"sections/dispatches/case/#problem-statement","level":2,"title":"Problem Statement","text":"<p>The problem and why is it important to solve ↠</p> <p>STATEMENT</p> <p>An organisation manually classifies trauma incidents for all the major trauma centres of five countries.  Per trauma case, an injury coding expert (a) examines the case's free and structured text, and assigns each piece of text to a category, and (b) assigns the case to a trauma category based on the combination text pieces &amp; categories detected; text pieces of the other/miscellaneous category are excluded from this exercise.  Trauma injury coding is an extremely intensive and time-consuming exercise, and injury coding error rates - per annum - can be quite high.  Hence, and as a first step, we are in search of a solution that automatically classifies text pieces vis-à-vis a set of provided categories.</p> <p></p>","path":["Token Classification:<br>Dispatches","Applications"],"tags":[]},{"location":"sections/dispatches/case/#outcome-expectations-underlying-aims","level":2,"title":"Outcome Expectations, Underlying Aims","text":"<p>The potential product's outcome expectations ↠</p> <p>The outcome expectations</p> <p>Real-time availability of classifications per trauma case.</p> <p>Underlying aims ↠</p> <p>Hence the underlying aims are</p> <p>The automatic classification of trauma case text pieces; objective →</p> <ul> <li>per case, automatic classification time &lt; 180 seconds.</li> <li>model metrics limits false negative rate ≤ 0.02, false positive rate ≤ 0.04</li> </ul> <p></p>","path":["Token Classification:<br>Dispatches","Applications"],"tags":[]},{"location":"sections/dispatches/case/#deployment-goal","level":2,"title":"Deployment Goal","text":"<p>A potential machine learning dependent project without a deployment goal is directionless, do not proceed without one.  </p> <p>A plausible deployment goal</p> <p></p> <p> </p> <p> </p> <p> </p> <p> </p>","path":["Token Classification:<br>Dispatches","Applications"],"tags":[]},{"location":"sections/dispatches/data/","level":1,"title":"Data","text":"<p>Info</p> <p>Visit data description, and data profile</p>","path":["Token Classification:<br>Dispatches","Data"],"tags":[]},{"location":"sections/dispatches/data/#sources","level":2,"title":"Sources","text":"<ul> <li>W-NUT 2017 (W-NUT 2017 Emerging and Rare entity recognition)<sup>1</sup>: Token Classification &amp; W-NUT 2017, get W-NUT 2017</li> <li>Few-NERD<sup>2</sup>: get Few-NERD</li> </ul> <ol> <li> <p>W-NUT: Workshop on Noisy User-generated Text ↩</p> </li> <li> <p>A Few-shot NERD (Named Entity Recognition Dataset) ↩</p> </li> </ol>","path":["Token Classification:<br>Dispatches","Data"],"tags":[]},{"location":"sections/dispatches/inventory/","level":1,"title":"Inventory","text":"<p>The membranes hub hosts the repositories of a token classification modelling project.  Readers may interact with the model via a simple open interface.</p> <p></p>","path":["Token Classification:<br>Dispatches","Inventory"],"tags":[]},{"location":"sections/dispatches/inventory/#repositories-etc","level":2,"title":"Repositories, etc.","text":"<p>The diagram outlines the functions of the git repositories.</p> <p></p> <p></p> <p></p> <p>The <code>text</code> package will be re-written, time and funding permitting.  The restructuring repository package is for restructuring the latest data captures in preparation for (a) human inspection, and (b) usage as a model re-training data source.</p> <p> </p> <p> </p> <p> </p> <p> </p>","path":["Token Classification:<br>Dispatches","Inventory"],"tags":[]},{"location":"sections/dispatches/models/","level":1,"title":"Models","text":"<p>Info</p> <p>Model Description</p> <p></p>","path":["Token Classification:<br>Dispatches","Models"],"tags":[]},{"location":"sections/dispatches/models/#approach","level":2,"title":"Approach","text":"<p>For a specific token classification dependent problem, an artificial intelligence engineer can</p> <ul> <li>Develop a token classification model per language model architecture, e.g., RoBERTa (Robustly Optimized BERT Pretraining Approach), ELECTRA (Efficiently Learning an Encoder that Classifies Token Replacements Accurately).  Per architecture, hyperparameter optimisation/tuning techniques, and libraries, aid the development of quite effective models, subject to early stopping, etc., constraints.</li> <li>Select the best model amongst the set of models; a single model per architecture.</li> </ul> <p></p>","path":["Token Classification:<br>Dispatches","Models"],"tags":[]},{"location":"sections/dispatches/models/#selecting-the-best-model","level":3,"title":"Selecting the best model","text":"<p>For this exercise, the best model was selected by comparing a testing phase metric, specifically Matthews Correlation Coefficient (MCC):</p> \\[MCC = \\frac{(tn \\bullet tp) - (fn \\bullet fp)}{{\\Large{[}}(tp + fp)(tp + fn)(tn + fp)(tn + fn){\\Large{]}}^{0.5}}\\] \\[MCC \\in [-1, \\quad +1]\\] <p>wherein tn, tp, fn, and fp denote true negative, true positive, false negative, and false positive, respectively.</p> <p></p>","path":["Token Classification:<br>Dispatches","Models"],"tags":[]},{"location":"sections/dispatches/models/#warning","level":3,"title":"Warning","text":"<p>Note, the best model of a set must undergo (a) mathematical evaluation, and (b) business/cost evaluation.  The latter is critical because an acceptable mathematical metric, e.g., \\(precision &gt; 0.9\\) does not necessarily lead to excellent business/cost metrics.</p> <p> </p> <p> </p> <p> </p> <p> </p> <p> </p> <p> </p> <p> </p> <p> </p>","path":["Token Classification:<br>Dispatches","Models"],"tags":[]},{"location":"sections/few/","level":1,"title":"Token Classification: Eclectic<sup>1</sup>","text":"<p>A second project focusing on text/string detection &amp; classification; cf. dispatches.  It fine-tunes a large language model architecture. Herein, the model depends on a different text tagging scheme, and a different data set; it also focuses on a different set of classes.  For more details visit the project's online hub.  The ultimate aim is Token Classification + Syntactic Parsing ↠ Knowledge Graph.</p> <p>The modelling package, and the supplementary packages, are adaptable components.</p> <p></p>","path":["Token Classification:<br>Eclectic","Token Classification: Eclectic"],"tags":[]},{"location":"sections/few/#in-brief","level":2,"title":"In brief","text":"CommentExpires / Decommissioning Intelligence Hub Hosts model &amp; data details, and [automatically updated] dynamic model card components. End of November 2025 Public Private Partnership Upcoming, an outline of a public private partnership building on this project. app A simple interface for interacting with the developed model; an endless number of interface designs are possible.  The classes in focus are art, building, event, geo-political entity, organisation, and weapon. Repositories Hub Hosts the project's git repositories. <ol> <li> <p>Herein, token classification is the detection and classification of text or strings via fine-tuned large language model architectures. Each model detects and classifies text or strings of vis-à-vis a set of classes. ↩</p> </li> </ol>","path":["Token Classification:<br>Eclectic","Token Classification: Eclectic"],"tags":[]},{"location":"sections/few/background/","level":1,"title":"Background","text":"","path":["Token Classification:<br>Eclectic","Background"],"tags":[]},{"location":"sections/few/background/#aim","level":2,"title":"Aim","text":"<p>The aim of this token classification project mirrors the aim of the dispatches token classification project.</p> <p> </p> <p> </p> <p> </p> <p> </p>","path":["Token Classification:<br>Eclectic","Background"],"tags":[]},{"location":"sections/hydrography/","level":1,"title":"River Levels","text":"<p>This project's focus is readily available river level intelligence, e.g., raw measure curves, river level forecasts, weighted rates of change, etc., in aid of resilience activity, preventive action, and more.  Initially, predominantly, for officials aiding Scottish Government Resilience activities.</p> <p></p>","path":["River Level<br>Intelligence","River Levels"],"tags":[]},{"location":"sections/hydrography/#in-brief","level":2,"title":"In brief","text":"CommentExpires / Decommissioning Intelligence Hub Hosts automatically updated measures, metrics, forecasts, etc. End of November 2025 BETA: LSTM Models Infrastructure funding ends at the end of November 2025, therefore the reinforcement learning models &amp; metrics, i.e., long short-term memory, might never exist beyond beta.       <ul> <li>Models: Each point estimates, and their training &amp; testing stage (a) point percentage errors distributions, (b) median percentage error, and (c) root median square error</li> <li>Inference</li> </ul> Future SEPA colleagues are investigating how to (a) build on, integrate the items of, this project, and (b) how to support the resilience team. Repositories Hub Hosts the project's git repositories.","path":["River Level<br>Intelligence","River Levels"],"tags":[]},{"location":"sections/hydrography/background/","level":1,"title":"Background","text":"","path":["River Level<br>Intelligence","Background"],"tags":[]},{"location":"sections/hydrography/background/#problem-statement","level":2,"title":"Problem Statement","text":"<p>The problem and why is it important to solve ↠</p> <p>STATEMENT</p> <p>The ability to anticipate problems is critical to the effectiveness of weather resilience teams.  In the case of Scotland's resilience team, an item of interest is river levels because unusual levels, or unusual river-level-rates-of-change, hint at possible or impending bank breaches.  At present</p> <ul> <li>During storms the team neither has access to real-time forecasts nor supplementary measures/metrics.</li> <li>Outwith storms the team does not have access to river levels intelligence, such intelligence enables hydrologists to identify problems, study patterns, etc., in-time.</li> </ul> <p>Hence, [in search of] an illustrative development of a plausible river levels intelligence hub, that hosts measures, metrics, actionable insights, predictions, etc., and exists within a continuous integration &amp; delivery setting.</p> <p> </p>","path":["River Level<br>Intelligence","Background"],"tags":[]},{"location":"sections/hydrography/background/#outcome-expectations-underlying-aims","level":2,"title":"Outcome Expectations, Underlying Aims","text":"<p>The potential product's outcome expectations ↠</p> <p>The outcome expectations during a weather warning period are frequent</p> <ol> <li>forecasting of river levels, that fall within a warning area.</li> <li>measures &amp; metrics updates; every three, or fewer, hours.</li> </ol> <p>The outcome expectations outwith a weather warning period are</p> <ol> <li>daily, within schedule, updates of measures &amp; metrics updates.</li> </ol> <p></p> <p>Underlying aims ↠</p> <p>Hence, the underlying aims are</p> <ol> <li>hourly-point forecasts/predictions of river levels, at least thirteen hours ahead; forecasting/predicting ahead every eleven or fewer hours.</li> <li>hourly-point forecasts/predictions within ± 0.025% error.</li> <li>the wherewithal to update measures &amp; metrics, and actions thereof, every three, or fewer, hours</li> </ol> <p> </p>","path":["River Level<br>Intelligence","Background"],"tags":[]},{"location":"sections/hydrography/background/#deployment-goal","level":2,"title":"Deployment Goal","text":"<p>The deployment goal ↠</p> <p>A continuously &amp; automatically updated online intelligence hub ...</p> <p>\\(\\ldots\\) that hosts river level intelligence metrics and forecasts. Outwith a warning period, the metrics shall be updated daily, and should include, at least,</p> <ol> <li>Raw river levels per gauge, and annual river level pattern comparison per gauge</li> <li>Daily extrema &amp; medians</li> <li>Drift</li> <li>Weighted rates of change of river levels</li> <li>A comparison of percentage river level changes w.r.t gauges within the same catchment.</li> </ol> <p> </p> <p> </p> <p> </p> <p> </p>","path":["River Level<br>Intelligence","Background"],"tags":[]},{"location":"sections/hydrography/ci/","level":1,"title":"Continuous Integration, etc.","text":"<p>This page summarises the orchestrations that underpin the automatic delivery of products, depending on the context.</p>","path":["River Level<br>Intelligence","Continuous Integration, etc."],"tags":[]},{"location":"sections/hydrography/ci/#daily","level":2,"title":"Daily","text":"","path":["River Level<br>Intelligence","Continuous Integration, etc."],"tags":[]},{"location":"sections/hydrography/ci/#warning","level":2,"title":"Warning","text":"<p>This section outlines the warning period orchestrations.</p>","path":["River Level<br>Intelligence","Continuous Integration, etc."],"tags":[]},{"location":"sections/hydrography/ci/#continuous","level":3,"title":"Continuous","text":"","path":["River Level<br>Intelligence","Continuous Integration, etc."],"tags":[]},{"location":"sections/hydrography/ci/#forecasting","level":3,"title":"Forecasting","text":"<p>The configurations JSON outlines the forecasting arguments, including forecast hours ahead.  The scheduler's frequency can be set to eleven or fewer hours, especially if the hours ahead expectations will not be met otherwise.</p> <p></p> <p></p> <p> </p>","path":["River Level<br>Intelligence","Continuous Integration, etc."],"tags":[]},{"location":"sections/hydrography/ci/#beta-reinforcement-learning","level":2,"title":"Beta: Reinforcement Learning","text":"","path":["River Level<br>Intelligence","Continuous Integration, etc."],"tags":[]},{"location":"sections/hydrography/ci/#modelling","level":3,"title":"Modelling","text":"","path":["River Level<br>Intelligence","Continuous Integration, etc."],"tags":[]},{"location":"sections/hydrography/ci/#inference","level":3,"title":"Inference","text":"","path":["River Level<br>Intelligence","Continuous Integration, etc."],"tags":[]},{"location":"sections/hydrography/data/","level":1,"title":"Data","text":"","path":["River Level<br>Intelligence","Data"],"tags":[]},{"location":"sections/hydrography/data/#sources","level":2,"title":"Sources","text":"<p>The data sources:</p> Notes SEPA Hydrometric Data SEPA API →application programming interface documentation, query service, water levels [1, 2], access controls, river level data quality codes, SEPA Environmental Data, geospatial standards register, gauge height, gauge datum reference point Granular Catchment Polygons CEFAS: <ul><li>Catchment Boundaries Pages ↠ 1, 2</li><li>Application Programming Interface Details</li></ul> Care HomesA raw gazetteer of Scotland's care homes is available via Scotland's Spatial Hub. For programmatic retrieval   <pre>\n    {file.name} = Care_Homes_for_Older_People_-_Scotland\n    url = \n      \"https://geo.spatialhub.scot/geoserver/sh_chep/wfs\n        ?service=WFS\n          &amp;authkey={authentication.key}\n            &amp;request=GetFeature\n              &amp;typeName=sh_chep:pub_chep\n                &amp;format_options=filename:{file.name}\n                  &amp;outputFormat=application/json\"\n  </pre> <p></p>","path":["River Level<br>Intelligence","Data"],"tags":[]},{"location":"sections/hydrography/data/#weather-warnings-meteorological-office","level":2,"title":"Weather Warnings &amp; Meteorological Office","text":"<p>Meteorological Office Warnings Services:</p> <ul> <li>RSS Feeds</li> <li>Electronic Mail Alerts</li> <li>National Severe Weather Warnings Service Public API</li> </ul> <p> </p> <p> </p> <p> </p> <p> </p>","path":["River Level<br>Intelligence","Data"],"tags":[]},{"location":"sections/hydrography/measures-metrics/","level":1,"title":"Measures, Metrics","text":"","path":["River Level<br>Intelligence","Measures, Metrics"],"tags":[]},{"location":"sections/hydrography/measures-metrics/#risks","level":2,"title":"Risks","text":"<p>This page outlines tools under investigation for real-time areas at risk identification.</p>","path":["River Level<br>Intelligence","Measures, Metrics"],"tags":[]},{"location":"sections/hydrography/measures-metrics/#rates-of-change-of-river-level","level":3,"title":"Rates of Change of River Level","text":"<p>Let</p> \\[\\mathcal{l}_{t}, \\: \\mathcal{l}_{t - \\tau}, \\: \\mathcal{l}_{t - 2\\tau}, \\: \\mathcal{l}_{t - 3\\tau}, \\: \\ldots\\] <p>represent the time series of a gauge's river level measures, separated by \\(\\tau\\) hours, and unit of measure ↠ millimetres. Then the weighted-rate-of-change, w.r.t. each \\(\\tau\\) hours interval, is</p> \\[r_{t - i\\tau} = \\frac{1}{\\tau} \\bigl(\\mathcal{l}_{t - i\\tau} - \\mathcal{l}_{t - (i + 1)\\tau}\\bigr) \\times \\frac{1}{\\mathcal{l}_{t - (i + 1)\\tau}} \\bigl(\\mathcal{l}_{t - i\\tau} - \\mathcal{l}_{t - (i + 1)\\tau}\\bigr) \\] <p>wherein</p> <ul> <li>\\(i = 0, 1, 2, 3, \\ldots\\)</li> <li>\\(t\\:\\) is a/the current time point.</li> <li>\\(r_{t - i\\tau}\\:\\) is the rate of change at time point \\(t - i\\tau\\), and its unit of measure is millimetres/hour.</li> </ul> <p>Noting that</p> \\[\\frac{1}{\\mathcal{l}_{t - (i + 1)\\tau}} \\bigl(\\mathcal{l}_{t - i\\tau} - \\mathcal{l}_{t - (i + 1)\\tau}\\bigr)\\] <p>determines the relative river level change w.r.t. consecutive river level values, i.e., values that are \\(\\tau\\) hours apart.</p> <p> </p>","path":["River Level<br>Intelligence","Measures, Metrics"],"tags":[]},{"location":"sections/hydrography/measures-metrics/#drift","level":2,"title":"Drift","text":"<p>For time series drift calculations, this hub depends on</p> <ul> <li>Jensen-Shannon Distance \\(J_{dist}\\)</li> <li>Wasserstein Distance \\(\\mathcal{W}\\)</li> </ul> <p>wherein</p> \\[J_{dist} = \\sqrt{J_{div}}\\] <p>\\(J_{div}\\) is the Jensen-Shannon Divergence, a method for determining the similarity of a pair of distributions<sup>1</sup>.  The similarity between a pair of distributions increases as \\(J_{dist} \\rightarrow 0\\).  Note, for a pair of distributions</p> \\[J_{div} \\in [0 \\quad 1]\\] <p>therefore</p> \\[J_{dist} \\in [0 \\quad 1]\\] <p>The Wasserstein Distance is a distance measure<sup>2</sup>.</p> <p>Important, do not consider scores in isolation, also consider the pattern of the scores over time.</p> <p> </p> <p> </p> <p> </p> <p> </p> <p> </p> <ol> <li> <p>Study Divergence Measures Based on Shannon Entropy for an in-depth understanding of Jensen-Shannon Divergence. ↩</p> </li> <li> <p>Part 6 of From GAN to WGAN has an in-depth discussion of the Wasserstein Distance.  GAN: Generative Adversarial Network, WGAN: Wasserstein Generative Adversarial Network ↩</p> </li> </ol>","path":["River Level<br>Intelligence","Measures, Metrics"],"tags":[]},{"location":"sections/hydrography/models/","level":1,"title":"Models","text":"<p>At present, river level prediction/forecasting depends on hourly river level data.  For the latest settings, including forecasting hours \\(\\small{ahead}\\), study the configuration files arguments.json | arguments.yaml; the YAML version includes definitions.</p> <p></p>","path":["River Level<br>Intelligence","Models"],"tags":[]},{"location":"sections/hydrography/models/#recurrent-neural-networks-lstm","level":2,"title":"Recurrent Neural Networks: LSTM","text":"<p>The latest model is a LSTM model.  For an indepth understanding study</p> <ul> <li>Long Short-Term Memory by Sepp Hochreiter, Jürgen Schmidhuber</li> <li>Understanding LSTM by Ralf C. Staudemeyer, Eric Rothstein Morris</li> </ul> <p>The project's LSTM modelling repository/package depends on tensorflow-keras libraries; applications, in practice<sup>2</sup><sup>, </sup><sup>3</sup><sup>, </sup><sup>4</sup>.</p> <p></p>","path":["River Level<br>Intelligence","Models"],"tags":[]},{"location":"sections/hydrography/models/#bayesian-structural-time-series-variational-inference","level":2,"title":"Bayesian Structural Time Series + Variational Inference","text":"<p>A Bayesian Structural Time Series (STS) algorithm is a state space algorithm, in brief</p> \\[y_{t} = \\pmb{x}^{T}_{t}\\pmb{\\beta}_{t} + \\epsilon_{t} \\qquad \\qquad \\qquad 1\\] \\[\\pmb{\\beta}_{t} = \\mathbf{F}_{t}\\pmb{\\beta}_{t - 1} + \\pmb{\\varsigma}_{t} \\qquad \\qquad \\quad 2\\] \\[\\epsilon_{t} \\sim \\mathcal{N}\\bigl(0, \\: \\sigma^{2}_{t}  \\bigr) \\qquad \\qquad \\qquad\\] \\[\\pmb{\\varsigma}_{t} \\sim \\mathcal{N}\\bigl(\\mathbf{0}, \\: \\pmb{\\mathcal{Z}}_{t}\\bigr) \\qquad \\qquad \\qquad\\] <p>whereby</p> description \\(y_{t}\\) \\(1 \\times 1\\) scalar.  Herein, it is a gauge's river level measure at time point \\(t\\). \\(\\pmb{x}_{t}\\) \\(p \\times 1\\).  A design vector. \\(\\pmb{\\beta}_{t}\\) \\(p \\times 1\\).  A state vector. \\(\\epsilon_{t}\\) \\(1 \\times 1\\) scalar.  An observation error, observation innovation. \\(\\mathbf{F}_{t}\\) \\(p \\times p\\).  A transition matrix. \\(\\pmb{\\varsigma}_{t}\\) \\(q \\times 1\\). A system error, or state innovation.<sup>1</sup> <p></p> <p>Formally, Eq. 1  is the observation model, whilst Eq. 2 is the transition or state model.  The latter models the transition of a state from \\(t - 1\\) to \\(t\\).</p> <p>A key advantage of state space modelling is \\(\\rightarrow\\) modelling via the superimposition of behaviours.  The superimposition, encoding, of behaviours occurs via the components \\(\\pmb{x}_{t}\\) &amp; \\(\\mathbf{F}_{t}\\).  For an in-depth outline, study Bayesian Inference of State Space Models by K. Triantafyllopoulos.</p> <p>In practice, model development is via TensorFlow Probability libraries.  Visit the project's river level modelling repository; the modelling arguments are readable with or without comments/definitions.</p> <p> </p> <p> </p> <p> </p> <p> </p> <ol> <li> <p>For more about the structure options of \\(\\pmb{\\varsigma}_{t}\\), i.e., system errors, study Inferring causal impact using Bayesian structural time-series models, and Bayesian Inference of State Space Models ↩</p> </li> <li> <p>Working with RNN ↩</p> </li> <li> <p>Timeseries forecasting for weather prediction; cf. ↩</p> </li> <li> <p>Traffic forecasting using graph neural networks and LSTM ↩</p> </li> </ol>","path":["River Level<br>Intelligence","Models"],"tags":[]}]}